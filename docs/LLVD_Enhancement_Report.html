<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>LLVD — Pixel Enhancement for Vehicle Detection Under Adverse Conditions</title>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap');

    :root {
      --primary: #1a365d;
      --primary-light: #2b6cb0;
      --accent: #2c5282;
      --bg: #ffffff;
      --text: #1a202c;
      --text-light: #4a5568;
      --border: #e2e8f0;
      --highlight: #ebf8ff;
      --code-bg: #f7fafc;
      --table-header: #1a365d;
      --table-stripe: #f7fafc;
      --diagram-bg: #edf2f7;
      --success: #276749;
      --warning: #c05621;
    }

    @page {
      size: A4;
      margin: 20mm 18mm 22mm 18mm;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Inter', -apple-system, sans-serif;
      color: var(--text);
      background: var(--bg);
      line-height: 1.8;
      font-size: 11.5pt;
      max-width: 210mm;
      margin: 0 auto;
      padding: 15mm 20mm;
    }

    /* Title Page */
    .title-page {
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      min-height: 85vh;
      text-align: center;
      page-break-after: always;
      border: 3px solid var(--primary);
      padding: 40px;
      margin-bottom: 30px;
    }

    .title-page .institution {
      font-size: 13pt;
      color: var(--text-light);
      letter-spacing: 2px;
      text-transform: uppercase;
      margin-bottom: 40px;
    }

    .title-page h1 {
      font-size: 24pt;
      color: var(--primary);
      font-weight: 700;
      line-height: 1.3;
      margin-bottom: 15px;
    }

    .title-page .subtitle {
      font-size: 14pt;
      color: var(--primary-light);
      font-weight: 400;
      margin-bottom: 50px;
    }

    .title-page .meta {
      font-size: 11pt;
      color: var(--text-light);
      line-height: 2;
    }

    .title-page .divider {
      width: 80px;
      height: 3px;
      background: var(--primary);
      margin: 30px auto;
    }

    /* Headings */
    h2 {
      font-size: 16pt;
      color: var(--primary);
      font-weight: 700;
      margin-top: 35px;
      margin-bottom: 12px;
      padding-bottom: 6px;
      border-bottom: 2.5px solid var(--primary);
      page-break-after: avoid;
    }

    h2 .num {
      color: var(--primary-light);
      margin-right: 8px;
    }

    h3 {
      font-size: 12.5pt;
      color: var(--accent);
      font-weight: 600;
      margin-top: 22px;
      margin-bottom: 8px;
      page-break-after: avoid;
    }

    h4 {
      font-size: 11.5pt;
      font-weight: 600;
      margin-top: 14px;
      margin-bottom: 6px;
      page-break-after: avoid;
    }

    /* Section wrapper — keeps heading + first content together */
    .section {
      page-break-before: always;
    }

    .section.no-page-break {
      page-break-before: auto;
    }

    p {
      margin-bottom: 10px;
      text-align: justify;
      orphans: 3;
      widows: 3;
    }

    /* Tables */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 14px 0 18px 0;
      font-size: 10.5pt;
      page-break-inside: avoid;
    }

    table th {
      background: var(--table-header);
      color: white;
      font-weight: 600;
      padding: 9px 12px;
      text-align: left;
      font-size: 10pt;
      letter-spacing: 0.3px;
    }

    table td {
      padding: 8px 12px;
      border-bottom: 1px solid var(--border);
      vertical-align: top;
    }

    table tr:nth-child(even) td {
      background: var(--table-stripe);
    }

    table caption {
      font-size: 10pt;
      font-style: italic;
      color: var(--text-light);
      margin-bottom: 6px;
      text-align: left;
      caption-side: top;
    }

    /* Block Diagrams */
    .diagram {
      background: var(--diagram-bg);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 24px;
      margin: 18px 0;
      page-break-inside: avoid;
    }

    .diagram-title {
      font-size: 10.5pt;
      font-weight: 600;
      color: var(--primary);
      text-align: center;
      margin-bottom: 16px;
    }

    .flow {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 4px;
    }

    .flow-block {
      background: white;
      border: 2px solid var(--primary);
      border-radius: 6px;
      padding: 10px 20px;
      text-align: center;
      font-size: 10pt;
      font-weight: 500;
      width: 85%;
      position: relative;
    }

    .flow-block.highlight {
      background: var(--highlight);
      border-color: var(--primary-light);
      border-width: 2.5px;
    }

    .flow-block.condition {
      background: #fffaf0;
      border-color: var(--warning);
      border-style: dashed;
    }

    .flow-block small {
      display: block;
      font-weight: 400;
      color: var(--text-light);
      font-size: 9pt;
      margin-top: 3px;
    }

    .flow-arrow {
      font-size: 16pt;
      color: var(--primary-light);
      line-height: 1;
    }

    .flow-row {
      display: flex;
      gap: 10px;
      width: 85%;
      justify-content: center;
    }

    .flow-row .flow-block {
      width: 48%;
    }

    /* Code blocks */
    code {
      font-family: 'JetBrains Mono', monospace;
      font-size: 9.5pt;
      background: var(--code-bg);
      padding: 1px 5px;
      border-radius: 3px;
      border: 1px solid var(--border);
    }

    pre {
      background: var(--code-bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 14px 16px;
      font-family: 'JetBrains Mono', monospace;
      font-size: 9.5pt;
      line-height: 1.6;
      overflow-x: auto;
      margin: 10px 0 16px 0;
      page-break-inside: avoid;
    }

    pre code {
      background: none;
      border: none;
      padding: 0;
    }

    /* Equation blocks */
    .equation {
      background: var(--highlight);
      border-left: 4px solid var(--primary-light);
      padding: 12px 18px;
      margin: 12px 0;
      font-family: 'JetBrains Mono', monospace;
      font-size: 10.5pt;
      text-align: center;
      page-break-inside: avoid;
    }

    /* Lists */
    ul,
    ol {
      margin: 8px 0 12px 24px;
    }

    li {
      margin-bottom: 4px;
      orphans: 2;
      widows: 2;
    }

    /* Figure captions */
    .fig-caption {
      font-size: 10pt;
      font-style: italic;
      color: var(--text-light);
      text-align: center;
      margin-top: 6px;
    }

    /* Highlight box */
    .highlight-box {
      background: var(--highlight);
      border: 1px solid #bee3f8;
      border-radius: 6px;
      padding: 14px 18px;
      margin: 14px 0;
      page-break-inside: avoid;
    }

    .highlight-box strong {
      color: var(--primary);
    }

    /* References */
    .ref-list {
      font-size: 10pt;
    }

    .ref-list li {
      margin-bottom: 6px;
      line-height: 1.6;
    }

    /* Page break hints */
    .page-break {
      page-break-before: always;
    }

    /* Keep-together blocks */
    .no-break {
      page-break-inside: avoid;
    }

    /* Print styles */
    @media print {
      body {
        padding: 0;
        max-width: 100%;
        font-size: 11pt;
      }

      .title-page {
        min-height: 92vh;
      }

      h2 {
        margin-top: 24px;
      }

      h3 {
        page-break-after: avoid;
      }

      h4 {
        page-break-after: avoid;
      }

      .no-break {
        page-break-inside: avoid;
      }

      table {
        page-break-inside: avoid;
      }

      .diagram {
        page-break-inside: avoid;
      }

      .highlight-box {
        page-break-inside: avoid;
      }

      .equation {
        page-break-inside: avoid;
      }

      pre {
        page-break-inside: avoid;
      }

      ol,
      ul {
        page-break-inside: avoid;
      }

      p {
        orphans: 3;
        widows: 3;
      }
    }

    /* TOC */
    .toc {
      margin: 20px 0 30px 0;
      page-break-after: always;
    }

    .toc h2 {
      border: none;
      text-align: center;
      font-size: 18pt;
    }

    .toc ul {
      list-style: none;
      margin: 20px 0;
      padding: 0;
    }

    .toc>ul>li {
      padding: 8px 0;
      border-bottom: 1px dotted var(--border);
      font-size: 11.5pt;
      font-weight: 500;
    }

    .toc ul ul {
      margin: 4px 0 4px 24px;
    }

    .toc ul ul li {
      border: none;
      padding: 3px 0;
      font-weight: 400;
      font-size: 10.5pt;
      color: var(--text-light);
    }

    .toc .page-num {
      float: right;
      color: var(--text-light);
    }
  </style>
</head>

<body>

  <!-- ============================== TITLE PAGE ============================== -->
  <div class="title-page">
    <div class="institution">Lattice Layer Vehicle Detection</div>
    <div class="divider"></div>
    <h1>Pixel Enhancement Techniques for Vehicle Detection Under Adverse Environmental Conditions</h1>
    <p class="subtitle">Fog Compensation, Rain Removal, and Low-Light Enhancement<br>using Classical Computer Vision</p>
    <div class="divider"></div>
    <div class="meta">
      <strong>Project:</strong> LLVD — Lattice Layer Vehicle Detection<br>
      <strong>Module:</strong> Enhanced Pipeline (<code>enhanced_pipeline.py</code>)<br>
      <strong>Date:</strong> February 2026<br>
    </div>
  </div>

  <!-- ============================== TABLE OF CONTENTS ============================== -->
  <div class="toc">
    <h2>Table of Contents</h2>
    <ul>
      <li>1. Introduction <span class="page-num">3</span></li>
      <li>2. Existing Works <span class="page-num">3</span>
        <ul>
          <li>2.1 Image Dehazing Techniques</li>
          <li>2.2 Rain Removal Methods</li>
          <li>2.3 Low-Light Enhancement</li>
          <li>2.4 Adaptive Detection Systems</li>
        </ul>
      </li>
      <li>3. Need for the Work <span class="page-num">4</span></li>
      <li>4. Problem Statement <span class="page-num">5</span></li>
      <li>5. Proposed Model <span class="page-num">5</span>
        <ul>
          <li>5.1 System Architecture</li>
          <li>5.2 Scene Classification Module</li>
          <li>5.3 Fog Compensation Module</li>
          <li>5.4 Rain Streak Removal Module</li>
          <li>5.5 Low-Light Enhancement Module</li>
          <li>5.6 Colour-Preserving Display</li>
          <li>5.7 Adaptive Detection Parameters</li>
          <li>5.8 Fast Mode Optimisation</li>
        </ul>
      </li>
      <li>6. Current Status — Working Demo <span class="page-num">8</span></li>
      <li>7. Data Used <span class="page-num">8</span></li>
      <li>8. Results <span class="page-num">9</span></li>
      <li>9. Future Work with Timeline <span class="page-num">10</span></li>
      <li>10. References <span class="page-num">10</span></li>
    </ul>
  </div>

  <!-- ============================== 1. INTRODUCTION ============================== -->
  <div class="section">
    <h2><span class="num">1.</span> Introduction</h2>

    <p>
      Intelligent Transportation Systems (ITS) rely on accurate, real-time vehicle detection and traffic monitoring for
      applications including congestion management, signal optimisation, incident detection, and urban planning.
      Camera-based vehicle detection using classical computer vision — specifically frame differencing and grid-based
      occupancy analysis — offers a cost-effective alternative to hardware sensors such as inductive loops and radar.
    </p>
    <p>
      However, the performance of pixel-level detection algorithms degrades significantly under adverse environmental
      conditions. Fog introduces atmospheric scattering that reduces contrast and visibility. Rain generates
      high-frequency vertical streaks that create false positives. Night and dusk lighting produce dark, noisy frames
      where vehicles become indistinguishable from the background.
    </p>
    <p>
      The <strong>Lattice Layer Vehicle Detection (LLVD)</strong> system addresses these challenges through a suite of
      pixel enhancement techniques that are conditionally applied based on real-time scene classification. The enhanced
      pipeline preprocesses each frame to normalise contrast, remove atmospheric degradation, and suppress noise —
      ensuring that the downstream detection module receives clean, high-quality inputs regardless of weather or
      lighting conditions.
    </p>

    <div class="highlight-box">
      <strong>Key Contribution:</strong> A unified enhancement pipeline that integrates Dark Channel Prior dehazing,
      rain streak removal, gamma correction, CLAHE, and edge-preserving denoising into a single adaptive system —
      achieving identical vehicle counts across conditions while offering a 23× speedup in fast mode (279 FPS vs 12
      FPS).
    </div>

  </div>
  <!-- ============================== 2. EXISTING WORKS ============================== -->
  <div class="section no-page-break">
    <h2><span class="num">2.</span> Existing Works</h2>

    <h3>2.1 Image Dehazing Techniques</h3>
    <p>
      He et al. (2009) introduced the <strong>Dark Channel Prior (DCP)</strong>, which observes that in most outdoor
      haze-free patches, at least one colour channel has very low intensity. This prior enables estimation of
      atmospheric light and a transmission map for haze removal. Subsequent work by He et al. (2013) refined the
      transmission map using <strong>guided filtering</strong>, producing edge-aligned results with lower computational
      cost than soft matting. Zhu et al. (2015) proposed the <strong>Colour Attenuation Prior (CAP)</strong>, modelling
      depth from the difference between brightness and saturation. Berman et al. (2016) introduced a non-local dehazing
      approach using haze-line regularisation.
    </p>

    <h3>2.2 Rain Removal Methods</h3>
    <p>
      Kang et al. (2012) decomposed rainy images into low-frequency (background) and high-frequency (rain + texture)
      components using morphological component analysis. Chen and Hsu (2013) leveraged the directional characteristics
      of rain streaks using guided filtering. More recent learning-based approaches by Fu et al. (2017) use deep detail
      networks, though classical spatial filtering remains effective for real-time applications where computational cost
      is critical.
    </p>

    <h3>2.3 Low-Light Enhancement</h3>
    <p>
      Histogram equalisation is the foundational technique for contrast enhancement. Pizer et al. (1987) proposed
      <strong>Adaptive Histogram Equalisation (AHE)</strong>, with the contrast-limited variant <strong>CLAHE</strong>
      (Zuiderveld, 1994) preventing noise amplification by clipping the histogram before redistribution. Gamma
      correction applies a non-linear power-law transform and is widely used in image processing pipelines. Bilateral
      filtering (Tomasi and Manduchi, 1998) provides edge-preserving smoothing by weighting neighbours based on both
      spatial distance and intensity similarity.
    </p>

    <h3>2.4 Adaptive Detection Systems</h3>
    <p>
      Most existing vehicle detection systems apply fixed preprocessing regardless of conditions. Adaptive systems that
      classify scenes and adjust processing accordingly are less common. Luo et al. (2015) proposed a weather-aware
      traffic monitoring framework, but relied on deep learning for weather classification. The LLVD approach uses
      lightweight classical metrics (brightness, dark channel, edge directionality) for scene classification, enabling
      conditional enhancement without neural network overhead.
    </p>

    <table>
      <caption>Table 1: Comparison of Enhancement Approaches</caption>
      <tr>
        <th>Approach</th>
        <th>Technique</th>
        <th>Strength</th>
        <th>Limitation</th>
      </tr>
      <tr>
        <td>He et al. (2009)</td>
        <td>Dark Channel Prior</td>
        <td>Strong dehazing quality</td>
        <td>Computationally expensive</td>
      </tr>
      <tr>
        <td>Zuiderveld (1994)</td>
        <td>CLAHE</td>
        <td>Adaptive, noise-controlled</td>
        <td>Single-channel only</td>
      </tr>
      <tr>
        <td>Kang et al. (2012)</td>
        <td>Rain decomposition</td>
        <td>Effective streak separation</td>
        <td>Complex, slow</td>
      </tr>
      <tr>
        <td>Fu et al. (2017)</td>
        <td>Deep Detail Net</td>
        <td>High quality</td>
        <td>Requires GPU + training data</td>
      </tr>
      <tr>
        <td><strong>LLVD (Proposed)</strong></td>
        <td><strong>Unified adaptive pipeline</strong></td>
        <td><strong>Real-time, condition-aware</strong></td>
        <td>Classical CV limits</td>
      </tr>
    </table>

  </div>
  <!-- ============================== 3. NEED FOR THE WORK ============================== -->
  <div class="section">
    <h2><span class="num">3.</span> Need for the Work</h2>

    <p>
      The base LLVD pipeline performs grid-based vehicle detection using frame differencing on a fixed set of Regions of
      Interest (ROIs). While effective under clear daylight conditions, the system exhibited the following failure
      modes:
    </p>

    <table>
      <caption>Table 2: Failure Modes Under Adverse Conditions</caption>
      <tr>
        <th>Condition</th>
        <th>Observed Problem</th>
        <th>Root Cause</th>
        <th>Impact on Detection</th>
      </tr>
      <tr>
        <td>Fog / Haze</td>
        <td>Vehicles blend into background</td>
        <td>Atmospheric scattering reduces contrast between foreground and background</td>
        <td>Missed detections (false negatives)</td>
      </tr>
      <tr>
        <td>Rain</td>
        <td>Ghost detections in empty cells</td>
        <td>Vertical rain streaks create pixel differences between frames</td>
        <td>False detections (false positives)</td>
      </tr>
      <tr>
        <td>Night</td>
        <td>Only headlights detected</td>
        <td>Low brightness, high sensor noise, insufficient contrast</td>
        <td>Partial detections, fragmentation</td>
      </tr>
      <tr>
        <td>Dusk / Dawn</td>
        <td>Inconsistent detections</td>
        <td>Transitional lighting, shadows, glare</td>
        <td>Intermittent false positives/negatives</td>
      </tr>
    </table>

    <p>
      These conditions are not edge cases — they occur daily during normal traffic monitoring. A robust detection system
      must handle fog, rain, and variable lighting without manual intervention. The need for an automated,
      condition-aware enhancement pipeline is therefore critical for deploying LLVD in real-world scenarios.
    </p>

  </div>
  <!-- ============================== 4. PROBLEM STATEMENT ============================== -->
  <div class="section no-page-break">
    <h2><span class="num">4.</span> Problem Statement</h2>

    <div class="highlight-box">
      <p>
        <strong>To develop a pixel-level image enhancement pipeline that automatically detects adverse environmental
          conditions (fog, rain, night, dusk) and conditionally applies targeted preprocessing techniques to improve
          vehicle detection accuracy in a grid-based frame differencing system, while maintaining real-time processing
          capability.</strong>
      </p>
    </div>

    <p>The problem encompasses the following sub-objectives:</p>
    <ol>
      <li><strong>Scene Classification</strong> — Automatically determine lighting condition (day/dusk/night), fog
        presence, and rain presence from each frame using lightweight classical metrics.</li>
      <li><strong>Fog Compensation</strong> — Remove atmospheric haze to restore contrast between vehicles and the road
        surface.</li>
      <li><strong>Rain Removal</strong> — Suppress rain streaks that cause false motion detection between consecutive
        frames.</li>
      <li><strong>Low-Light Enhancement</strong> — Brighten dark frames, enhance contrast, and reduce sensor noise for
        night and dusk conditions.</li>
      <li><strong>Performance Optimisation</strong> — Provide a fast processing mode suitable for real-time deployment
        without sacrificing detection accuracy.</li>
    </ol>

  </div>
  <!-- ============================== 5. PROPOSED MODEL ============================== -->
  <div class="section">
    <h2><span class="num">5.</span> Proposed Model</h2>

    <h3>5.1 System Architecture</h3>

    <p>The enhanced pipeline operates as a preprocessing stage inserted between frame capture and the existing
      grid-based detection module. The architecture follows a conditional branching design — enhancements are applied
      only when the scene classifier determines they are necessary.</p>

    <div class="diagram">
      <div class="diagram-title">Figure 1: Complete System Architecture — Enhanced Pipeline</div>
      <div class="flow">
        <div class="flow-block"><strong>VIDEO INPUT</strong><small>Frame capture from traffic camera</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block highlight"><strong>SCENE CLASSIFICATION</strong><small>Brightness analysis · Dark Channel
            fog estimation · Laplacian + Sobel rain detection</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-row">
          <div class="flow-block condition"><strong>FOG?</strong><small>fog_index > 0.6</small></div>
          <div class="flow-block condition"><strong>RAIN?</strong><small>lap_std > 35 AND vert_ratio > 1.5</small></div>
        </div>
        <div class="flow-arrow">▼</div>
        <div class="flow-row">
          <div class="flow-block highlight"><strong>DCP DEHAZING</strong><small>Dark Channel Prior haze removal</small>
          </div>
          <div class="flow-block highlight"><strong>RAIN REMOVAL</strong><small>Median blur + directional
              morphology</small></div>
        </div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block"><strong>GRAYSCALE CONVERSION</strong><small>BGR → Grayscale</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block highlight"><strong>GAMMA CORRECTION</strong><small>Pre-computed LUT: γ=2.2 (night) ·
            γ=1.5 (dusk)</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block highlight"><strong>CLAHE EQUALISATION</strong><small>Clip=4.0 (night) · Clip=2.5 (dusk) ·
            Standard (day)</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block highlight"><strong>BILATERAL / GAUSSIAN DENOISING</strong><small>Edge-preserving noise
            reduction for low-light</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block"><strong>FRAME DIFFERENCING + GRID DETECTION</strong><small>absdiff → adaptive threshold
            → morphology → occupancy grid</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block"><strong>TRACKING · COUNTING · SPEED ESTIMATION</strong><small>Centroid tracker ·
            Line-crossing counter · Displacement-based speed</small></div>
      </div>
    </div>

    <h3>5.2 Scene Classification Module</h3>

    <p>The scene classifier analyses each frame (once per batch for efficiency) and returns three condition flags:
      lighting type, fog presence, and rain presence.</p>

    <h4>Lighting Classification</h4>
    <p>The mean pixel brightness of the grayscale frame determines the lighting category:</p>

    <div class="equation">
      brightness = mean(frame_gray)
    </div>

    <table>
      <caption>Table 3: Lighting Classification Thresholds</caption>
      <tr>
        <th>Condition</th>
        <th>Brightness Range</th>
        <th>Triggered Enhancements</th>
      </tr>
      <tr>
        <td>Night</td>
        <td>brightness &lt; 50</td>
        <td>Gamma (γ=2.2), CLAHE (clip=4.0), bilateral denoising</td>
      </tr>
      <tr>
        <td>Dusk</td>
        <td>50 ≤ brightness &lt; 120</td>
        <td>Gamma (γ=1.5), CLAHE (clip=2.5), bilateral denoising</td>
      </tr>
      <tr>
        <td>Day</td>
        <td>brightness ≥ 120</td>
        <td>Standard histogram equalisation</td>
      </tr>
    </table>

    <h4>Fog Detection — Dark Channel Prior</h4>
    <p>The dark channel of the grayscale frame is computed via morphological erosion with a 15×15 kernel. In fog-free
      images, the dark channel tends toward zero (due to shadows and dark objects). In foggy images, scattering raises
      these values uniformly.</p>

    <div class="equation">
      fog_index = 1.0 − mean(dark_channel) / 255.0 &nbsp;&nbsp;|&nbsp;&nbsp; is_foggy = (fog_index > 0.6)
    </div>

    <h4>Rain Detection — Laplacian Variance + Edge Directionality</h4>
    <p>Rain detection uses a dual-metric approach. First, the Laplacian standard deviation captures overall
      high-frequency content. Second, Sobel filters in vertical and horizontal directions measure edge directionality —
      rain streaks produce dominant vertical edges.</p>

    <div class="equation">
      is_rainy = (std(Laplacian) > 35.0) AND (mean|Sobel_v| / mean|Sobel_h| > 1.5)
    </div>

    <h3>5.3 Fog Compensation Module</h3>

    <h4>Dark Channel Prior Dehazing (Normal Mode)</h4>
    <p>The atmospheric scattering model describes image degradation in fog as:</p>

    <div class="equation">
      I(x) = J(x) · t(x) + A · (1 − t(x))
    </div>

    <p>Where <code>I(x)</code> is the observed hazy image, <code>J(x)</code> is the desired scene radiance,
      <code>t(x)</code> is the transmission map, and <code>A</code> is the atmospheric light. The DCP implementation
      recovers J(x) through:</p>

    <ol>
      <li><strong>Dark channel computation</strong> — Minimum intensity across channels, eroded with a 15×15 kernel</li>
      <li><strong>Atmospheric light estimation</strong> — Average of top 0.1% brightest dark channel pixels in the
        original image</li>
      <li><strong>Transmission map</strong> — <code>t(x) = 1 − 0.95 · dark_channel_normalised(x)</code></li>
      <li><strong>Guided filter refinement</strong> — Edge-aligned smoothing using the grayscale image as guide
        (radius=40, ε=10⁻³)</li>
      <li><strong>Scene recovery</strong> — <code>J(x) = (I(x) − A) / max(t(x), 0.1) + A</code></li>
    </ol>

    <h4>Fast CLAHE Dehazing (Fast Mode)</h4>
    <p>For real-time processing, a lightweight alternative applies CLAHE directly on the V (brightness) channel of HSV,
      achieving approximately 80% of DCP's contrast improvement at 1/50th the computational cost:</p>

    <pre><code>hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)
h, s, v = cv2.split(hsv)
clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
v = clahe.apply(v)
result = cv2.cvtColor(cv2.merge([h, s, v]), cv2.COLOR_HSV2BGR)</code></pre>

    <h3>5.4 Rain Streak Removal Module</h3>
    <p>Rain streaks are mitigated through a two-step spatial filtering approach:</p>

    <ol>
      <li><strong>Median Filtering (5×5)</strong> — Suppresses thin, high-intensity rain streaks while preserving edges.
        Median filters are optimal for impulse-like noise patterns.</li>
      <li><strong>Directional Morphological Closing (15×1 horizontal kernel)</strong> — Fills residual vertical gaps.
        The horizontal orientation ensures that road markings and vehicle edges are not disrupted.</li>
    </ol>

    <p>Additionally, rain-specific detection parameters are applied downstream — a larger Gaussian blur kernel (9×9),
      lower binary threshold (50), and more aggressive dilation (4 iterations) to handle residual noise.</p>

    <table>
      <caption>Table 4: Rain-Adaptive Detection Parameters</caption>
      <tr>
        <th>Parameter</th>
        <th>Normal</th>
        <th>Rain-Adapted</th>
        <th>Rationale</th>
      </tr>
      <tr>
        <td>Blur kernel</td>
        <td>(5,5)</td>
        <td>(9,9)</td>
        <td>Stronger smoothing for residual streaks</td>
      </tr>
      <tr>
        <td>Binary threshold</td>
        <td>75</td>
        <td>50</td>
        <td>Lower threshold for reduced contrast</td>
      </tr>
      <tr>
        <td>Dilation iterations</td>
        <td>3</td>
        <td>4</td>
        <td>Merge fragmented detections</td>
      </tr>
    </table>

  </div>
  <div class="section">
    <h3>5.5 Low-Light Enhancement Module</h3>

    <h4>Gamma Correction</h4>
    <p>The non-linear intensity transformation brightens dark frames while avoiding over-exposure of bright regions:</p>

    <div class="equation">
      Output = 255 × (Input / 255)<sup>1/γ</sup>
    </div>

    <p>The gamma mapping is <strong>pre-computed as a 256-entry Look-Up Table</strong> at module load time. Runtime
      application reduces to a single <code>cv2.LUT()</code> call — an O(1) operation per pixel, making this enhancement
      essentially free.</p>

    <table>
      <caption>Table 5: Gamma Correction Values</caption>
      <tr>
        <th>Condition</th>
        <th>Gamma (γ)</th>
        <th>Effect</th>
      </tr>
      <tr>
        <td>Night</td>
        <td>2.2</td>
        <td>Strong brightening — dark regions significantly lifted</td>
      </tr>
      <tr>
        <td>Dusk</td>
        <td>1.5</td>
        <td>Moderate brightening — subtle lift without over-exposure</td>
      </tr>
      <tr>
        <td>Day</td>
        <td>1.0 (no change)</td>
        <td>Not applied</td>
      </tr>
    </table>

    <h4>CLAHE — Contrast Limited Adaptive Histogram Equalisation</h4>
    <p>Unlike global histogram equalisation which can amplify noise, CLAHE operates on 8×8 tiles with clip-limited
      histograms. The clip limit controls enhancement aggressiveness:</p>

    <table>
      <caption>Table 6: CLAHE Configuration</caption>
      <tr>
        <th>Parameter</th>
        <th>Night</th>
        <th>Dusk</th>
        <th>Day</th>
      </tr>
      <tr>
        <td>Method</td>
        <td>CLAHE</td>
        <td>CLAHE</td>
        <td>Standard equalizeHist</td>
      </tr>
      <tr>
        <td>Clip limit</td>
        <td>4.0</td>
        <td>2.5</td>
        <td>N/A</td>
      </tr>
      <tr>
        <td>Tile size</td>
        <td>8×8</td>
        <td>8×8</td>
        <td>N/A</td>
      </tr>
    </table>

    <h4>Noise Reduction</h4>
    <p>Low-light frames contain elevated sensor noise. Two strategies are provided:</p>
    <ul>
      <li><strong>Bilateral Filter (Normal Mode)</strong> — Edge-preserving smoothing with parameters d=5, σ_color=50,
        σ_space=50. Preserves vehicle edges while removing noise.</li>
      <li><strong>Gaussian Blur (Fast Mode)</strong> — 5×5 uniform smoothing, approximately 10× faster. Acceptable for
        detection since frame differencing is robust to mild uniform blurring.</li>
    </ul>

    <h3>5.6 Colour-Preserving Display Enhancement</h3>
    <p>The output video is enhanced by operating exclusively on the <strong>V (Value) channel of HSV</strong>,
      preserving the original hue and saturation. This ensures vehicles, road markings, and traffic lights retain their
      natural colours in the enhanced output, while brightness and contrast are improved.</p>

    <div class="diagram">
      <div class="diagram-title">Figure 2: Colour-Preserving Enhancement Flow</div>
      <div class="flow">
        <div class="flow-block"><strong>ROI (BGR)</strong></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block"><strong>BGR → HSV</strong><small>Split into H, S, V channels</small></div>
        <div class="flow-arrow">▼</div>
        <div class="flow-row">
          <div class="flow-block" style="opacity:0.5"><strong>H, S</strong><small>Preserved unchanged</small></div>
          <div class="flow-block highlight"><strong>V → Gamma → CLAHE → Denoise</strong><small>Brightness
              enhanced</small></div>
        </div>
        <div class="flow-arrow">▼</div>
        <div class="flow-block"><strong>Merge [H, S, V'] → HSV → BGR</strong><small>Colour-preserved output</small>
        </div>
      </div>
    </div>

    <h3>5.7 Adaptive Detection Parameters</h3>
    <p>The detection stage uses condition-specific parameters optimised for each scene type:</p>

    <table>
      <caption>Table 7: Condition-Specific Detection Parameters</caption>
      <tr>
        <th>Condition</th>
        <th>Blur Kernel</th>
        <th>Binary Threshold</th>
        <th>Dilation Iters</th>
        <th>Morph Close</th>
      </tr>
      <tr>
        <td>Day</td>
        <td>(5,5)</td>
        <td>75</td>
        <td>3</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Dusk</td>
        <td>(5,5)</td>
        <td>40</td>
        <td>3</td>
        <td>No</td>
      </tr>
      <tr>
        <td>Night</td>
        <td>(5,5)</td>
        <td>30</td>
        <td>4</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td>Fog</td>
        <td>(7,7)</td>
        <td>30–40</td>
        <td>4</td>
        <td>Yes</td>
      </tr>
      <tr>
        <td>Rain</td>
        <td>(9,9)</td>
        <td>50</td>
        <td>4</td>
        <td>Yes</td>
      </tr>
    </table>

    <h3>5.8 Fast Mode Optimisation</h3>
    <p>A <code>--fast</code> CLI flag enables a performance-optimised mode that replaces expensive operations with
      lightweight alternatives:</p>

    <div class="diagram">
      <div class="diagram-title">Figure 3: Normal Mode vs Fast Mode</div>
      <div class="flow">
        <div class="flow-row">
          <div class="flow-block highlight"><strong>NORMAL MODE</strong><small>Full quality</small></div>
          <div class="flow-block" style="border-color: var(--success);"><strong>FAST MODE</strong><small>Optimised for
              speed</small></div>
        </div>
        <div class="flow-row" style="margin-top:8px">
          <div class="flow-block">DCP Dehazing<small>~50ms/ROI</small></div>
          <div class="flow-block" style="border-color: var(--success);">CLAHE on V Channel<small>~1ms/ROI</small></div>
        </div>
        <div class="flow-row" style="margin-top:4px">
          <div class="flow-block">Bilateral Filter<small>~5ms/ROI</small></div>
          <div class="flow-block" style="border-color: var(--success);">Gaussian Blur<small>~0.5ms/ROI</small></div>
        </div>
        <div class="flow-row" style="margin-top:4px">
          <div class="flow-block">Full Display Enhancement<small>DCP + HSV pipeline</small></div>
          <div class="flow-block" style="border-color: var(--success);">Skipped<small>Raw frames + overlays</small>
          </div>
        </div>
      </div>
    </div>

  </div>
  <!-- ============================== 6. CURRENT STATUS ============================== -->
  <div class="section">
    <h2><span class="num">6.</span> Current Status — Working Demo</h2>

    <p>The enhanced pipeline is fully implemented and operational. The system has been tested on traffic surveillance
      video containing DUSK + FOG conditions and produces the following outputs:</p>

    <ul>
      <li><strong>Output Video</strong> — <code>output/videos/output_enhanced_cv.mp4</code> with grid overlays, tracking
        IDs, speed labels, and vehicle counts</li>
      <li><strong>JSON Report</strong> — Machine-readable detection results with timestamps</li>
      <li><strong>Markdown Report</strong> — Human-readable summary of execution metrics</li>
    </ul>

    <p><strong>Execution commands:</strong></p>
    <pre><code># Normal mode (full enhancement)
python run.py --pipeline enhanced

# Fast mode (optimised for speed)
python run.py --pipeline enhanced --fast</code></pre>

    <p>The system correctly identifies the scene as <strong>DUSK + FOG</strong> (brightness=109, fog_index=0.65) and
      automatically applies dehazing, gamma correction (γ=1.5), CLAHE (clip=2.5), and bilateral denoising. Vehicle
      tracking with centroid matching, line-crossing counting, and displacement-based speed estimation operate on the
      enhanced frames.</p>

  </div>
  <!-- ============================== 7. DATA USED ============================== -->
  <div class="section no-page-break">
    <h2><span class="num">7.</span> Data Used</h2>

    <table>
      <caption>Table 8: Dataset Specifications</caption>
      <tr>
        <th>Parameter</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>Video source</td>
        <td>Traffic surveillance camera footage</td>
      </tr>
      <tr>
        <td>Total frames</td>
        <td>1,659 frames</td>
      </tr>
      <tr>
        <td>Frame rate</td>
        <td>25 FPS</td>
      </tr>
      <tr>
        <td>Scene condition</td>
        <td>Dusk + Fog</td>
      </tr>
      <tr>
        <td>Brightness (mean)</td>
        <td>109</td>
      </tr>
      <tr>
        <td>Fog index</td>
        <td>0.65</td>
      </tr>
      <tr>
        <td>ROI 1 (Lane 1)</td>
        <td>Position: (545, 159), Size: 284×140 pixels</td>
      </tr>
      <tr>
        <td>ROI 2 (Lane 2)</td>
        <td>Position: (238, 161), Size: 284×140 pixels</td>
      </tr>
      <tr>
        <td>Grid configuration</td>
        <td>7 rows × 14 columns per ROI (98 cells)</td>
      </tr>
      <tr>
        <td>Colour channel</td>
        <td>Grayscale (fast-path enabled)</td>
      </tr>
    </table>

  </div>
  <!-- ============================== 8. RESULTS ============================== -->
  <div class="section">
    <h2><span class="num">8.</span> Results</h2>

    <h3>8.1 Performance Comparison</h3>

    <table>
      <caption>Table 9: Normal Mode vs Fast Mode — Measured Performance (1,659 frames, DUSK + FOG)</caption>
      <tr>
        <th>Metric</th>
        <th>Normal Mode</th>
        <th>Fast Mode</th>
        <th>Improvement</th>
      </tr>
      <tr>
        <td>Execution time</td>
        <td>135.63s</td>
        <td>5.94s</td>
        <td><strong>~23× faster</strong></td>
      </tr>
      <tr>
        <td>Processing speed</td>
        <td>12.23 FPS</td>
        <td>279.15 FPS</td>
        <td><strong>~23× faster</strong></td>
      </tr>
      <tr>
        <td>Memory usage</td>
        <td>54.62 MB</td>
        <td>53.37 MB</td>
        <td>Similar</td>
      </tr>
      <tr>
        <td>Workers</td>
        <td>11</td>
        <td>11</td>
        <td>Same</td>
      </tr>
    </table>

    <h3>8.2 Detection Accuracy</h3>

    <table>
      <caption>Table 10: Vehicle Detection Results — Normal vs Fast Mode</caption>
      <tr>
        <th>Metric</th>
        <th>Normal Mode</th>
        <th>Fast Mode</th>
        <th>Variance</th>
      </tr>
      <tr>
        <td>Lane 1 vehicle count</td>
        <td>10</td>
        <td>10</td>
        <td><strong>Identical</strong></td>
      </tr>
      <tr>
        <td>Lane 2 vehicle count</td>
        <td>20</td>
        <td>20</td>
        <td><strong>Identical</strong></td>
      </tr>
      <tr>
        <td>Combined count</td>
        <td>30</td>
        <td>30</td>
        <td><strong>Identical</strong></td>
      </tr>
      <tr>
        <td>Avg density (Lane 1)</td>
        <td>0.0603</td>
        <td>0.0607</td>
        <td>0.7%</td>
      </tr>
      <tr>
        <td>Avg density (Lane 2)</td>
        <td>0.0667</td>
        <td>0.0694</td>
        <td>4.0%</td>
      </tr>
      <tr>
        <td>Avg speed (Lane 1)</td>
        <td>46.8 km/h</td>
        <td>51.2 km/h</td>
        <td>~9%</td>
      </tr>
      <tr>
        <td>Avg speed (Lane 2)</td>
        <td>40.3 km/h</td>
        <td>44.4 km/h</td>
        <td>~10%</td>
      </tr>
      <tr>
        <td>Total IDs assigned</td>
        <td>147</td>
        <td>—</td>
        <td>—</td>
      </tr>
    </table>

    <div class="highlight-box">
      <strong>Key Finding:</strong> Vehicle counts are <strong>identical</strong> across both modes, confirming that the
      fast mode optimisations preserve detection accuracy. The ~9-10% speed estimation variance is within acceptable
      bounds, caused by the slightly different dehazing approach affecting pixel displacement calculations.
    </div>

    <h3>8.3 Scene Classification Results</h3>

    <table>
      <caption>Table 11: Scene Classification Output</caption>
      <tr>
        <th>Detected Condition</th>
        <th>Batches</th>
        <th>Enhancements Applied</th>
      </tr>
      <tr>
        <td>Dusk + Fog</td>
        <td>26 (all batches)</td>
        <td>DCP/CLAHE dehazing, gamma (γ=1.5), CLAHE (clip=2.5), bilateral/Gaussian denoise</td>
      </tr>
    </table>

  </div>
  <!-- ============================== 9. FUTURE WORK ============================== -->
  <div class="section">
    <h2><span class="num">9.</span> Future Work with Timeline</h2>

    <table>
      <caption>Table 12: Planned Enhancements and Timeline</caption>
      <tr>
        <th>Phase</th>
        <th>Work Item</th>
        <th>Description</th>
        <th>Timeline</th>
      </tr>
      <tr>
        <td rowspan="2"><strong>Phase 1</strong><br><em>Short-term</em></td>
        <td>GPU Acceleration</td>
        <td>Port DCP dehazing, bilateral filter, and CLAHE to CUDA using <code>cv2.cuda</code> module for real-time
          normal mode processing</td>
        <td>Mar 2026</td>
      </tr>
      <tr>
        <td>Multi-Video Testing</td>
        <td>Validate enhancement pipeline across diverse scenes — heavy rain, dense fog, pure night, and mixed
          conditions</td>
        <td>Mar 2026</td>
      </tr>
      <tr>
        <td rowspan="2"><strong>Phase 2</strong><br><em>Mid-term</em></td>
        <td>Snow and Glare Handling</td>
        <td>Add snow detection using texture analysis and glare suppression using specular highlight removal</td>
        <td>Apr 2026</td>
      </tr>
      <tr>
        <td>Deep Learning Hybrid</td>
        <td>Integrate a lightweight CNN (e.g., MobileNet) for scene classification to improve accuracy over hand-crafted
          metrics</td>
        <td>Apr–May 2026</td>
      </tr>
      <tr>
        <td rowspan="2"><strong>Phase 3</strong><br><em>Long-term</em></td>
        <td>Temporal Consistency</td>
        <td>Apply temporal smoothing to scene classification to prevent flickering between conditions across frames</td>
        <td>May 2026</td>
      </tr>
      <tr>
        <td>Deployment-Ready Packaging</td>
        <td>Docker containerisation, REST API for remote video processing, and dashboard for live monitoring</td>
        <td>Jun 2026</td>
      </tr>
    </table>

  </div>
  <!-- ============================== 10. REFERENCES ============================== -->
  <div class="section no-page-break">
    <h2><span class="num">10.</span> References</h2>

    <ol class="ref-list">
      <li>He, K., Sun, J. and Tang, X. (2009). <em>Single Image Haze Removal Using Dark Channel Prior.</em> IEEE
        Transactions on Pattern Analysis and Machine Intelligence, 33(12), pp.2341–2353.</li>
      <li>He, K., Sun, J. and Tang, X. (2013). <em>Guided Image Filtering.</em> IEEE Transactions on Pattern Analysis
        and Machine Intelligence, 35(6), pp.1397–1409.</li>
      <li>Zuiderveld, K. (1994). <em>Contrast Limited Adaptive Histogram Equalization.</em> In: Graphics Gems IV.
        Academic Press, pp.474–485.</li>
      <li>Pizer, S.M., Amburn, E.P., Austin, J.D. et al. (1987). <em>Adaptive Histogram Equalization and Its
          Variations.</em> Computer Vision, Graphics, and Image Processing, 39(3), pp.355–368.</li>
      <li>Tomasi, C. and Manduchi, R. (1998). <em>Bilateral Filtering for Gray and Color Images.</em> Proceedings of the
        IEEE International Conference on Computer Vision (ICCV), pp.839–846.</li>
      <li>Kang, L.W., Lin, C.W. and Fu, Y.H. (2012). <em>Automatic Single-Image-Based Rain Streaks Removal via Image
          Decomposition.</em> IEEE Transactions on Image Processing, 21(4), pp.1742–1755.</li>
      <li>Chen, Y.L. and Hsu, C.T. (2013). <em>A Generalized Low-Rank Appearance Model for Spatio-Temporally Correlated
          Rain Streaks.</em> IEEE International Conference on Computer Vision (ICCV), pp.1968–1975.</li>
      <li>Fu, X., Huang, J., Zeng, D., Huang, Y., Ding, X. and Paisley, J. (2017). <em>Removing Rain from Single Images
          via a Deep Detail Network.</em> IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
        pp.3855–3863.</li>
      <li>Zhu, Q., Mai, J. and Shao, L. (2015). <em>A Fast Single Image Haze Removal Algorithm Using Color Attenuation
          Prior.</em> IEEE Transactions on Image Processing, 24(11), pp.3522–3533.</li>
      <li>Berman, D., Treibitz, T. and Avidan, S. (2016). <em>Non-Local Image Dehazing.</em> IEEE Conference on Computer
        Vision and Pattern Recognition (CVPR), pp.1674–1682.</li>
      <li>OpenCV Documentation. (2024). <em>OpenCV: Image Processing.</em> Available at: https://docs.opencv.org/</li>
    </ol>
  </div>

</body>

</html>