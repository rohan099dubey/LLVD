# Traffic System Analysis â€” Technical Audit & Expansion Roadmap

> **Prepared for**: Traffic System Analysis Platform Team
> **Date**: 2026-02-17
> **Role**: Senior Computer Vision & Software Architect

---

# Part 1: Technical Audit & Documentation (The "Now")

---

## 1. System Overview â€” Current Codebase Architecture

The **Current Codebase** (`LLVD-latest`) is a classical computer-vision traffic analysis pipeline built entirely in Python. It contains two independent processing implementations:

### 1.1 Files

| File | Lines | Purpose |
|------|-------|---------|
| [rparallel_imp (1).py](file:///e:/WORK/additionalProject/LLVD-latest/rparallel_imp%20(1).py) | 863 | **Advanced Pipeline** â€” multiprocessing, tracking, speed estimation, fog/night adaptation |
| [honors_imp (1).py](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20(1).py) | 452 | **Base Pipeline** â€” multiprocessing, DBSCAN clustering, bounding-box generation |
| [user_input_data.json](file:///e:/WORK/additionalProject/LLVD-latest/user_input_data.json) | 8 | Runtime configuration (video path, color channel, grid dims) |

### 1.2 Architecture Diagram

```mermaid
graph TD
    subgraph Input
        A["Video / Image Sequence"]
        B["user_input_data.json"]
    end

    subgraph "Core Pipeline (rparallel_imp)"
        C["batch_roi_generator<br/>Batch + Prefetch"]
        D["classify_scene<br/>Day/Dusk/Night/Fog"]
        E["worker_task (Pool)<br/>Per-frame ROI processing"]
        F["process_single_roi<br/>Adaptive equalization + diff"]
        G["process_grid_vectorized<br/>Motion detection grid"]
        H["find_connected_components<br/>Vehicle blob detection"]
        I["CentroidTracker.update<br/>ID assignment + speed"]
        J["Counting Line Check"]
        K["Visualization + AsyncVideoWriter"]
    end

    subgraph Output
        L["output_advanced_cv.mp4"]
        M["Profile Logs (JSON)"]
        N["Console Stats"]
    end

    A --> C
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
    H --> I
    I --> J
    J --> K
    K --> L
    K --> M
    K --> N
```

### 1.3 Problems the Current System Solves

| Capability | Implementation | File |
|---|---|---|
| **Vehicle Detection** | Frame-differencing â†’ grid-based motion â†’ connected components | `rparallel_imp` |
| **Traffic Density** | Ratio of occupied grid cells to total cells, per-lane | Both files |
| **Vehicle Counting** | Centroid tracker + counting line crossing | `rparallel_imp` |
| **Speed Estimation** | Pixel displacement Ã— calibration factor â†’ km/h (EMA smoothed) | `rparallel_imp` |
| **Scene Adaptation** | Day/dusk/night classification + fog detection via Dark Channel Prior | `rparallel_imp` |
| **Dehazing** | Dark Channel Prior + guided filter refinement | `rparallel_imp` |
| **Vehicle Clustering** | DBSCAN on active grid cells â†’ bounding boxes | `honors_imp` |
| **Parallel Processing** | `multiprocessing.Pool` with batched frame pairs | Both files |

### 1.4 Key Design Decisions

- **Lattice/Grid-based detection** instead of contour-based: The frame is divided into a configurable grid (e.g., 7Ã—14), and each cell is independently checked for motion via pixel threshold counting. This is the "Lattice Layer" concept.
- **Frame differencing** (not background subtraction): Motion is detected by `cv2.absdiff(frame_n, frame_n+1)`, making the system stateless across frames.
- **Hardcoded ROI coordinates**: Two fixed regions (`ROI1 = (545, 159, 284, 140)` and `ROI2 = (238, 161, 284, 140)`) corresponding to two lanes.
- **Grayscale fast-path**: When [gray](file:///e:/WORK/additionalProject/Lattice-Layer-Vehicle-Detection-/seq.py#114-123) or `V` channel is selected, an optimized single-channel path is used.
- **Async video writing**: A dedicated writer thread with a queue decouples frame writing from processing.
- **Function-level profiling**: A custom `@time_function` decorator tracks exclusive wall-clock time, merged across worker processes via JSON logs.

---

## 2. Legacy vs. Current â€” Evolution Analysis

### 2.1 Side-by-Side Comparison

| Aspect | Legacy (`Lattice-Layer-Vehicle-Detection-`) | Current (`LLVD-latest`) |
|---|---|---|
| **Parallelism** | `ThreadPoolExecutor` (GIL-bound) | `multiprocessing.Pool` (true parallelism) |
| **Grid Processing** | Per-cell contour finding (`findContours`) | Vectorized `reshape + sum` (10-50Ã— faster) |
| **Histogram Eq.** | Basic `cv2.equalizeHist` | Adaptive CLAHE (night/dusk) + standard (day) |
| **Scene Awareness** | None | Day/dusk/night + fog via Dark Channel Prior |
| **Dehazing** | None | DCP-based dehazing with guided filter |
| **Vehicle Tracking** | None | Centroid-based tracker with ID persistence |
| **Vehicle Counting** | None â€” density only | Counting-line crossing detection |
| **Speed Estimation** | None | Pixel-to-meter + EMA smoothing |
| **Clustering** | None in parallel ver.; DBSCAN in sequential | Connected components (OpenCV); DBSCAN in `honors_imp` |
| **Input Support** | [.mp4](file:///e:/WORK/additionalProject/Lattice-Layer-Vehicle-Detection-/seq_output.mp4)/`.avi` files only | Video files **+ image sequences** (folder of JPGs) |
| **Output** | Annotated video + optional Excel | Annotated video + JSON profiling logs |
| **GUI** | Tkinter-based GUI | Removed (CLI-only) |
| **Configuration** | JSON with `execution_mode` field | JSON without mode field (parallel-only) |
| **Profiling** | `cProfile` | Custom `@time_function` decorator with cross-process merge |
| **ROI Coordinates** | [(680,350,200,180)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) / [(360,350,200,180)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) | [(545,159,284,140)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) / [(238,161,284,140)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) |
| **Binary Threshold** | 20 | 75 (day), 30 (night), 40 (dusk/fog) |
| **Grid Default** | 6Ã—6 | 7Ã—14 |

### 2.2 What Changed and Why

```mermaid
graph LR
    subgraph "Legacy (v1)"
        L1["ThreadPool<br/>GIL-limited"]
        L2["findContours<br/>per grid cell"]
        L3["No tracking"]
        L4["Fixed params"]
        L5["Tkinter GUI"]
    end

    subgraph "Current (v2)"
        C1["multiprocessing.Pool<br/>true parallelism"]
        C2["Vectorized grid<br/>reshape + sum"]
        C3["CentroidTracker<br/>ID + speed + count"]
        C4["Adaptive params<br/>scene-aware"]
        C5["CLI + profiling"]
    end

    L1 -->|"Perf fix"| C1
    L2 -->|"10-50Ã— speedup"| C2
    L3 -->|"New feature"| C3
    L4 -->|"Robustness"| C4
    L5 -->|"Stripped for batch"| C5
```

> [!IMPORTANT]
> The most impactful architectural change was the migration from `ThreadPoolExecutor` to `multiprocessing.Pool`. Python's GIL makes threads ineffective for CPU-bound OpenCV operations. The legacy code was essentially running single-threaded despite using a thread pool.

---

## 3. Data Stream Analysis â€” MVI_39761

### 3.1 Dataset Overview

| Property | Value |
|---|---|
| **Source** | UA-DETRAC benchmark dataset |
| **Sequence** | MVI_39761 |
| **Total Frames** | 1,660 sequential JPEGs |
| **Resolution** | 960 Ã— 540 pixels |
| **File Size** | ~43â€“48 KB per frame |
| **Estimated FPS** | 25 fps |
| **Duration** | ~66 seconds |

### 3.2 Visual Analysis â€” Sample Frames

````carousel
![Frame 1 â€” Low-traffic, dusk conditions, wide multi-lane highway](C:/Users/rohan duby/.gemini/antigravity/brain/dea6cf1b-95f4-4801-bcc8-334535817506/sample_frame_001.jpg)
<!-- slide -->
![Frame 400 â€” Moderate traffic, mixed vehicles (bus, sedan), headlights active](C:/Users/rohan duby/.gemini/antigravity/brain/dea6cf1b-95f4-4801-bcc8-334535817506/sample_frame_400.jpg)
<!-- slide -->
![Frame 900 â€” Sparse traffic, near-empty road, low contrast](C:/Users/rohan duby/.gemini/antigravity/brain/dea6cf1b-95f4-4801-bcc8-334535817506/sample_frame_900.jpg)
````

### 3.3 Data Challenges Identified

| Challenge | Severity | Description |
|---|---|---|
| **Dusk/Twilight Lighting** | ðŸ”´ High | Overcast sky with low ambient light. Vehicles rely on headlights/taillights, creating high dynamic range situations. Pavement appears low-contrast gray. |
| **Headlight Glare** | ðŸŸ¡ Medium | Oncoming vehicles produce bright spots that can generate false motion detection, especially on wet-looking road surface. |
| **Road Surface Reflections** | ðŸŸ¡ Medium | The pavement shows specular reflections from headlights, producing ghost detections in frame-differencing. |
| **Perspective Distortion** | ðŸŸ¡ Medium | Camera is elevated (overpass view). Vehicles far away are tiny (~10-15 px), while near vehicles are larger (~80-100 px). This affects tracking accuracy. |
| **Multi-Lane Width** | ðŸŸ¡ Medium | ~6 lanes in each direction. The current system uses only 2 fixed ROIs, potentially missing lanes. |
| **Mixed Vehicle Types** | ðŸŸ¢ Low | Sedans, taxis, buses, trucks, motorcycles present. Important for future classification features. |
| **Sparse-to-Moderate Density** | ðŸŸ¢ Low | Traffic is never heavily congested in this sequence, reducing occlusion challenges. |
| **Static Background** | ðŸŸ¢ Low | Trees and buildings are relatively static, reducing false positives from background motion. |

### 3.4 How the Current System Handles This Data

- **CLAHE + scene classification** adaptively boosts contrast in the ROI for dusk conditions, mitigating the low-light challenge.
- **Dark Channel Prior** detects potential fog/haze (the overcast sky could trigger mild fog detection).
- **Adaptive thresholding** (75 for day, 30 for night, 40 for dusk) helps avoid false positives from headlight glare.
- **Hardcoded ROI limitation**: The fixed ROI coordinates [(545,159,284,140)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) and [(238,161,284,140)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) cover only a portion of the 960Ã—540 frame, likely targeting two specific lanes rather than the full road width.
- **Grid resolution** (7Ã—14 = 98 cells) provides fine-grained spatial detection within each ROI.

> [!WARNING]
> The hardcoded ROI coordinates were likely tuned for a different camera angle. For MVI_39761 (UA-DETRAC), these coordinates may not perfectly align with the actual lane positions. **This is the biggest immediate risk** when running the current system on this data.

---

# Part 2: The Expansion Plan (The "Future")

---

## 4. Proposed Architecture for the Add-On

### 4.1 Design Philosophy

The add-on should follow a **modular microservice-inspired architecture** that wraps the existing pipeline as a "Detection Core" and builds new capabilities as independent, pluggable modules.

### 4.2 Target Architecture

```mermaid
graph TB
    subgraph "Input Layer"
        I1["Video File / Image Seq"]
        I2["RTSP/IP Camera Stream"]
        I3["API Upload"]
    end

    subgraph "Preprocessing Layer"
        P1["Frame Decoder<br/>(OpenCV / FFmpeg)"]
        P2["ROI Auto-Detection<br/>(Lane line detection)"]
        P3["Scene Classifier<br/>(Day/Night/Fog/Rain)"]
        P4["Frame Stabilizer"]
    end

    subgraph "Detection Core (Existing + Enhanced)"
        D1["Lattice Grid Detector<br/>(Current system)"]
        D2["DNN Object Detector<br/>(YOLOv8 / RT-DETR)"]
        D3["Detector Fusion<br/>(ensemble or switch)"]
    end

    subgraph "Analytics Layer (New Add-On)"
        A1["Multi-Object Tracker<br/>(ByteTrack / BoT-SORT)"]
        A2["Vehicle Classifier<br/>(Sedan/Truck/Bus/Moto)"]
        A3["Speed Estimator<br/>(Homography-calibrated)"]
        A4["ANPR Module<br/>(OCR on plate crops)"]
        A5["Anomaly Detector<br/>(Stopped vehicle / wrong-way)"]
        A6["Flow Optimizer<br/>(Signal timing suggestions)"]
    end

    subgraph "Output Layer"
        O1["Annotated Video Stream"]
        O2["REST API / WebSocket"]
        O3["Dashboard (Web UI)"]
        O4["Alerts & Notifications"]
        O5["Database / Data Lake"]
    end

    I1 --> P1
    I2 --> P1
    I3 --> P1
    P1 --> P2
    P1 --> P3
    P1 --> P4
    P2 --> D1
    P3 --> D1
    P4 --> D2
    D1 --> D3
    D2 --> D3
    D3 --> A1
    A1 --> A2
    A1 --> A3
    A1 --> A4
    A1 --> A5
    A3 --> A6
    A1 --> O1
    A2 --> O2
    A3 --> O2
    A5 --> O4
    A6 --> O3
    A1 --> O5
```

### 4.3 Key Architectural Principles

| Principle | Rationale |
|---|---|
| **Detector-agnostic tracking** | The tracker should accept bounding boxes from ANY detector (grid-based, YOLO, or both). This lets you swap/upgrade detectors without rewriting tracking. |
| **Config-driven ROI** | All ROI coordinates must come from config or auto-detection. Zero hardcoded pixel values. |
| **Pipeline as directed graph** | Each stage is a node with defined inputs/outputs. Stages can be enabled/disabled per deployment. |
| **Gradual DNN integration** | Don't abandon the lattice detector. Instead, run it alongside a DNN detector and fuse results. The lattice detector is fast and lightweight â€” ideal as a "first-pass" filter. |
| **Calibration registry** | Each camera has a calibrated homography matrix, pixel-per-meter ratio, and lane mask stored in a config file. |

---

## 5. Feasibility Check â€” Advanced Features

### Given the MVI_39761 data quality, here is an honest assessment:

| Feature | Feasibility | Confidence | Notes |
|---|---|---|---|
| **Vehicle Counting** | âœ… Already works | ðŸŸ¢ High | Current centroid tracker handles this. Accuracy can improve with DNN detections. |
| **Traffic Density** | âœ… Already works | ðŸŸ¢ High | Grid-based density calculation is solid. |
| **Speed Estimation** | âš ï¸ Partially works | ðŸŸ¡ Medium | Current pixel-based speed is uncalibrated. Needs camera homography calibration for real-world accuracy. At 960Ã—540 resolution, there's enough pixel detail for reasonable estimates. |
| **Vehicle Classification** | âœ… Feasible | ðŸŸ¢ High | Vehicles are distinguishable (buses vs. sedans vs. motorcycles). YOLOv8 can classify at this resolution reliably. |
| **ANPR (License Plates)** | âš ï¸ Challenging | ðŸ”´ Low | At 960Ã—540 from an overpass, license plates are **10-20 pixels wide** â€” far below the ~100px minimum needed for reliable OCR. Would require a dedicated plate-reader camera at lane level. |
| **Accident/Anomaly Detection** | âœ… Feasible | ðŸŸ¡ Medium | Stopped vehicles and wrong-way driving are detectable via trajectory analysis. Actual collision detection is very difficult from this viewpoint. |
| **Flow Optimization** | âœ… Feasible | ðŸŸ¡ Medium | Requires accumulating density + speed data over time and modeling signal timing. The data pipeline exists; the optimization model is the new work. |
| **Wrong-Way Detection** | âœ… Feasible | ðŸŸ¢ High | Easy to detect: track direction vs. expected lane direction. |
| **Queue Length Estimation** | âœ… Feasible | ðŸŸ¢ High | Count consecutive occupied cells from a stop line. |

> [!CAUTION]
> **ANPR is NOT feasible** with this camera setup. The resolution and distance make plate characters unreadable. If ANPR is a hard requirement, a separate close-range camera system must be budgeted for.

---

## 6. Step-by-Step Roadmap â€” Phased Approach

### Phase 0: Foundation Cleanup (2-3 weeks)

> **Goal**: Make the current codebase production-grade before adding features.

| # | Task | Priority | Effort |
|---|---|---|---|
| 0.1 | **Remove hardcoded ROIs** â€” replace with config file or JSON defining per-camera ROI polygons | ðŸ”´ Critical | 2 days |
| 0.2 | **Add CLI argument parser** â€” accept config path, video path, output path as CLI args | ðŸŸ¡ Medium | 1 day |
| 0.3 | **Create `requirements.txt`** + proper project structure (`src/`, [config/](file:///e:/WORK/additionalProject/Lattice-Layer-Vehicle-Detection-/gui.py#91-93), `output/`, `tests/`) | ðŸŸ¡ Medium | 1 day |
| 0.4 | **Add unit tests** for [classify_scene](file:///e:/WORK/additionalProject/LLVD-latest/rparallel_imp%20%281%29.py#155-172), [process_grid_vectorized](file:///e:/WORK/additionalProject/LLVD-latest/rparallel_imp%20%281%29.py#383-412), [CentroidTracker](file:///e:/WORK/additionalProject/LLVD-latest/rparallel_imp%20%281%29.py#267-381), [find_connected_components](file:///e:/WORK/additionalProject/LLVD-latest/rparallel_imp%20%281%29.py#235-265) | ðŸŸ¡ Medium | 3 days |
| 0.5 | **Fix file naming** â€” remove [(1)](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#290-443) from filenames, establish clean module names | ðŸŸ¢ Low | 0.5 days |
| 0.6 | **Add [VideoReader](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#14-66) class** from `honors_imp` into the main pipeline (or create shared `io_utils.py`) | ðŸŸ¡ Medium | 1 day |
| 0.7 | **Document ROI calibration procedure** â€” how to determine ROI for a new camera | ðŸŸ¡ Medium | 1 day |

---

### Phase 1: DNN Detection Integration (3-4 weeks)

> **Goal**: Add a modern object detector alongside the existing lattice detector.

| # | Task | Priority | Effort |
|---|---|---|---|
| 1.1 | **Integrate YOLOv8-nano** (ultralytics) for vehicle detection â€” inference only, no training | ðŸ”´ Critical | 3 days |
| 1.2 | **Create `DetectorInterface` ABC** â€” [detect(frame) â†’ List[BBox]](file:///e:/WORK/additionalProject/LLVD-latest/honors_imp%20%281%29.py#81-109) â€” implemented by both lattice and YOLO | ðŸ”´ Critical | 2 days |
| 1.3 | **Build `DetectorFusion` module** â€” union/intersection of lattice + YOLO results | ðŸŸ¡ Medium | 3 days |
| 1.4 | **Add vehicle classification** â€” YOLOv8 already provides class labels (car, truck, bus, motorcycle) | ðŸŸ¡ Medium | 2 days |
| 1.5 | **Benchmark: Lattice vs. YOLO vs. Fused** â€” accuracy and speed on MVI_39761 | ðŸŸ¡ Medium | 3 days |
| 1.6 | **GPU acceleration** â€” add CUDA support for YOLO inference, keep lattice on CPU | ðŸŸ¡ Medium | 2 days |

> [!TIP]
> Use **YOLOv8-nano** initially â€” it runs at ~100+ FPS on a modern GPU and ~15 FPS on CPU. Only upgrade to YOLOv8-small/medium if accuracy is insufficient.

---

### Phase 2: Tracking & Counting Upgrade (2-3 weeks)

> **Goal**: Replace the basic centroid tracker with a production-grade tracker.

| # | Task | Priority | Effort |
|---|---|---|---|
| 2.1 | **Integrate ByteTrack** (or BoT-SORT) â€” handles occlusion, ID switches, and re-identification | ðŸ”´ Critical | 5 days |
| 2.2 | **Camera homography calibration** â€” map pixel coordinates to real-world coordinates for accurate speed/distance | ðŸ”´ Critical | 3 days |
| 2.3 | **Multi-lane counting line** â€” configurable per-lane counting lines instead of fixed 50% position | ðŸŸ¡ Medium | 2 days |
| 2.4 | **Direction-aware counting** â€” separate inbound vs. outbound counts | ðŸŸ¡ Medium | 1 day |
| 2.5 | **Calibrated speed estimation** â€” use homography matrix for real-world speed in km/h | ðŸŸ¡ Medium | 2 days |

---

### Phase 3: Analytics & Anomaly Detection (3-4 weeks)

> **Goal**: Build the intelligence layer that produces actionable insights.

| # | Task | Priority | Effort |
|---|---|---|---|
| 3.1 | **Stopped-vehicle detector** â€” flag vehicles with speed < 2 km/h for > 5 seconds | ðŸŸ¡ Medium | 3 days |
| 3.2 | **Wrong-way detector** â€” compare track direction against lane-expected direction | ðŸŸ¡ Medium | 2 days |
| 3.3 | **Queue length estimator** â€” count consecutive occupied cells from a reference point | ðŸŸ¡ Medium | 2 days |
| 3.4 | **Traffic flow rate** â€” vehicles/minute computed over sliding window | ðŸŸ¡ Medium | 1 day |
| 3.5 | **Occupancy rate** â€” percentage of time a lane segment is occupied | ðŸŸ¡ Medium | 1 day |
| 3.6 | **Alert system** â€” trigger notifications when anomalies are detected | ðŸŸ¢ Low | 3 days |
| 3.7 | **Time-series data export** â€” density, speed, count per 15-second interval to CSV/DB | ðŸŸ¡ Medium | 2 days |

---

### Phase 4: Dashboard & API (4-5 weeks)

> **Goal**: Expose the system via web interfaces for operators and integrators.

| # | Task | Priority | Effort |
|---|---|---|---|
| 4.1 | **REST API** (FastAPI) â€” `/analyze`, `/cameras`, `/alerts`, `/statistics` | ðŸŸ¡ Medium | 5 days |
| 4.2 | **WebSocket stream** â€” live annotated video to browser | ðŸŸ¡ Medium | 3 days |
| 4.3 | **Web dashboard** â€” real-time stats, historical charts, camera management | ðŸŸ¡ Medium | 8 days |
| 4.4 | **Database** â€” PostgreSQL + TimescaleDB for time-series traffic data | ðŸŸ¡ Medium | 3 days |
| 4.5 | **Multi-camera support** â€” run independent pipelines per camera, aggregate at API layer | ðŸŸ¡ Medium | 5 days |

---

### Phase 5 (Optional): Advanced Modules

| # | Task | Feasibility |
|---|---|---|
| 5.1 | **ANPR** â€” requires dedicated close-range camera hardware | âš ï¸ Hardware-dependent |
| 5.2 | **Flow optimization** â€” signal timing suggestions based on accumulated data | âœ… Feasible after Phase 3 |
| 5.3 | **Edge deployment** â€” NVIDIA Jetson / Raspberry Pi optimization | âœ… Feasible with YOLOv8-nano |
| 5.4 | **Multi-camera Re-ID** â€” track vehicles across camera views | âš ï¸ Research-grade |

---

## 7. Technical Debt Mitigation Strategy

| Risk | Mitigation |
|---|---|
| **Hardcoded ROIs** | Phase 0.1 â€” config-driven ROIs |
| **No tests** | Phase 0.4 â€” unit test suite |
| **Monolithic scripts** | Phase 0.3 â€” proper package structure with `src/detectors/`, `src/trackers/`, `src/analytics/` |
| **Unversioned models** | Use `ultralytics` model registry; pin model versions in config |
| **No containerization** | Add `Dockerfile` in Phase 4 |
| **GIL contention risk** | Already mitigated by `multiprocessing.Pool`; keep heavy compute in worker processes |

---

## 8. Recommended Technology Stack

| Layer | Technology | Rationale |
|---|---|---|
| **Detection** | YOLOv8 (ultralytics) + existing Lattice | Best tradeoff: speed, accuracy, ease of integration |
| **Tracking** | ByteTrack / Supervision | Production-proven, handles occlusion well |
| **Speed/Calibration** | OpenCV `solvePnP` + homography | Standard approach for traffic cameras |
| **API** | FastAPI | Async, fast, auto-docs, Python-native |
| **Dashboard** | Next.js or Vite + React | Modern, reactive, component-based |
| **Database** | PostgreSQL + TimescaleDB | Time-series optimized for traffic metrics |
| **Video Streaming** | WebRTC or MJPEG over WebSocket | Low-latency live view |
| **Deployment** | Docker + docker-compose | Reproducible, portable |
| **ML Framework** | PyTorch (via ultralytics) | YOLO ecosystem standard |

---

## 9. Summary & Recommendations

### Immediate Actions (This Week)
1. **Validate ROI coordinates** on MVI_39761 â€” run the current system and check if the ROIs align with actual lanes
2. **Clean up filenames** â€” rename to `main_pipeline.py` and `base_pipeline.py`
3. **Create project structure** with `requirements.txt`

### Medium-Term (1-2 Months)
4. Complete Phase 0 â†’ Phase 1 â†’ Phase 2
5. At the end of Phase 2, you'll have: YOLO-enhanced detection + ByteTrack + calibrated speed

### Long-Term (3-6 Months)
6. Complete Phase 3 â†’ Phase 4
7. At the end of Phase 4, you'll have: a full web-accessible traffic analytics platform

> [!NOTE]
> The existing lattice-grid approach is architecturally sound and computationally efficient. **Don't replace it â€” augment it.** It serves as an excellent lightweight "first-pass" detector that can run on embedded devices, while YOLO handles the heavy lifting on GPU-equipped systems.
